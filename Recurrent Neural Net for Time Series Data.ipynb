{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Recurrent neural networks are a special type of neural network which work especially well on certain types of data where a value at time $t$ is dependent on previous values, i.e. values at time $t^- < t$. This includes text data, where a word in a sentence depends on the words that come before it, and on time series data, such as stock  market data where the share value of a particular stock depends on its value the previous day.\n",
    "\n",
    "In a CNN all layers send information to the next layer. In an RNN, certain layers also loop back to themselves. \n",
    "\n",
    "In this notebook we'll predict the number of aeroplane passengers. To see how RNNs can be used on text data, you could also try the following tutorial https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "First you need to download the file `airline-passengers.csv` and upload it within google colab. On the very left side of the screen, there are 3 symbols. The bottom one is files. Click that and then upload the data to be able to access it within google colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "dataframe = read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "dataframe.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MinMaxScaler` is a **class** that allows us to scale the data between a certain range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that `scaler` is now an instance of the class `MinMaxScaler`, and we can call all functions within `MinMaxScaler` by using `scaler.function_name`. Lets use this to send the data to the range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = scaler.fit_transform(dataframe)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data into 67% training and 33% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to convert the data so there are two columns: one at time step $t$, one at timestep $t+1$. We will create a function to do this for us.\n",
    "\n",
    "You can think of this similar to how we have used supervision labels in the past, the first column is the input and the second column is what we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, time_diff=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_diff-1):\n",
    "        a = dataset[i:(i+time_diff), 0] #this is only really useful with time_diff > 1. For time_diff=1 we can just use i\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_diff, 0])\n",
    "        \n",
    "    return numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the function we have just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff = 1\n",
    "trainX, trainY = create_dataset(train, time_diff)\n",
    "testX, testY = create_dataset(test, time_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "[trainX[:5], trainY[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Implementation\n",
    "We will use a sequential model similar to the ones we explored in Practical 5. The difference is that we will add an LSTM layer. This makes the network a *Recurrent Neural Network*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "To make the network an RNN, we will implement an LSTM cell. LSTM stands for long-short term memory and allows the network to remember information from a long time ago if it believes that information is useful. I don't want to go into too many details as the structure is quite complicated. I want to focus on how we implement it into a neural network.\n",
    "\n",
    "The below model is similar to ones you have seen in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(1, time_diff)))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam') #Note that the loss is now mean squared error\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unscaling the data\n",
    "During the pre-processing stage, we scaled the data down to be between 0 and 1. Now to view the results, we need to scale it back up to its initial size. We can call the function `inverse_transform` within `scaler` to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "The remaining cells set up our predictions so that we can plot them onto the original graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[time_diff:len(trainPredict)+time_diff, :] = trainPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(time_diff*2)+1:len(dataset)-1, :] = testPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the green line (representing our test predictions) is very close to the actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "From next week, the practical sessions will be purely for project work. Google Colab have a large number of tutorials on how to use Colab for different applications at https://research.google.com/seedbank/. Some of these may be related to the projects you will undertake.\n",
    "\n",
    "Choose one that you feel is similar to your project and work through it. These are generally slightly more advanced than the material we have been through. You can ask me questions and I will try to help out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
